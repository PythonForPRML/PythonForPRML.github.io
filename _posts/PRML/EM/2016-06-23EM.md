---
layout: article
title:  "EM"
categories: prml
time: 2016-4-6
---


# 一、隐含变量

高斯混合分布可以看做多个高斯分布的线性叠加，其形式如下：

$$p(\mathbf{x})=\sum_{k=1}^{K} \pi_k N(\mathbf{x} \vert \mu_k ,\Sigma_k)$$

在此我们要引入一个采用1-of-K表示法的K维二项随机变量Z，其特定的的k个元素$$z_k$$为1，其他元素都为0。

比如z是5维的，其所有可能的取值为：

$$
z=(1,0,0,0,0)\\
z=(0,1,0,0,0)\\
z=(0,0,1,0,0)\\
z=(0,0,0,1,0)\\
z=(0,0,0,0,1)
$$

我们定义x，z的联合分布为$$p(x,z)$$，其可由z的边缘分布p(z)以及另一个条件分布$$p(x \vert z)$$得到。

另外，我们定义z的边缘分布为$$\pi_k$$,即：

$$p(z_k=1)=\pi_k$$

因为z使用的是1-of-K表示法，因此z的边缘分布可以写成

$$p(z)=\prod_{k=1}^{K}\pi_{k}^{z_k}$$

另外我们定义x关于z的条件分布为：

$$p(\mathbf{x} \vert z_k=1)=N(\mathbf{x} \vert \mu_k ,\Sigma_k)$$

由于z采用了1-of-K表示法，上式也可以改写成：

$$p(\mathbf{x} \vert z)=\prod_{k=1}^{K}N(\mathbf{x} \vert \mu_k , \Sigma_k)^{z_k}$$

x和z的联合分布$$p(x,z)=p(z)p(x \vert z)$$,因此x的边缘分布可以由将所有可能z值的联合分布相加得到：

$$p(\mathbf{x})=\sum_z p(z)p(\mathbf{x} \vert z)=\sum_{k=1}^{K}\pi_k N(\mathbf{x} \vert \mu_k ,\Sigma_k)$$

- 在这里，我们将z成为隐含变量，我们为高斯混合找到了一种等价的含有显式的隐含变量表示法。这种表示方法在后面会显示出极大的便捷性，我们将在EM算法中详细说明。

- 对于每一个观察值$$\mathbf{x}$$都有一个z与之对应

# 二、responsibility——每一个高斯分量对于观察值的贡献

另一个重要的量化标准是给定x时z的条件概率，我们用$$\gamma(z_k)$$表示$$p(z_k=1 \vert x)$$,根据贝叶斯理论：

$$\gamma(z_k)=p(z_k=1 \vert x)=\frac{p(z_k)=1p(x \vert z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(x \vert z_j =1)}\\
=\frac{\pi_k N(x \vert \mu_k , \Sigma_k)}{\sum_{j=1}^{K} \pi_j N(x \vert \mu_j , \Sigma_j)}
$$

$$\gamma(z_k)$$表示了对于观察值x，K个分量中第k个分量分别对其的贡献。


## 1.responsibility含义的图形化说明：

为了说明responsibility的含义，即K个分量重第k个分量对观察值的后验概率的贡献，我们可以使用模拟数据来说明：

首先根据z的概率分布p(z)生成z的一个值，记作$$\hat{z}$$,然后根据条件概率分布$$p(x \vert \hat{z})$$生成x的一个值。

那么根据上述这种生成数据的方式，我们可以用图形化来表示联合分布以及边缘分布。

### 1. x和z的联合分布

对于x和z的联合分布，我们首先在图中画出x（用一个点表示），然后根据该x是由哪一个$$\hat{z}$$值所生成，将该点标记为相应的颜色。如图9-5(a)所示。

 - 为了方便说明，可以假设z是一个3维的二项随机变量，那么z有三种不同的取值，根据z值的不同，x被标记为红、绿或者蓝色之一。（因为红绿蓝是三原色，根据不同的比例混合可以产生其他的颜色）。


<img src="/images/PRML/EM/image1.png"  >

### 2.x的边缘分布

对于x的边缘分布，直接在图中绘出x所表示的点，忽略z的影响即可，因此图中的点只有一种颜色。如图9-5(b)所示。

### 3.$$\gamma(z_k)$$——responsibility

每一个生成的数据点x由我们根据不同的z值`模拟`得到的，但是我们可以假设我们事先并不知道每一个数据点x是由哪一个z值生成的，我们来计算当观察到数据点x时它由各个z值所生成的概率，即z的后验概率，也就是$$\gamma(z_k)$$。

同样地，我们在图中画出每一个数据点x，对于每一个点的颜色，要根据各个z值得后验概率综合得到。

即对于k=1，2，3，$$\gamma(z_k)$$表示红绿蓝三种颜色的比例(某种颜色的三原色比例)。比如对于数据点$$x_n$$,$$\gamma(z_{1n})=1,\gamma(z_{2n})=0,\gamma(z_{3n})=0$$，那么这个点的颜色为红色。如果数据点$$x_n$$,$$\gamma(z_{1n})=0,\gamma(z_{2n})=0.5,\gamma(z_{3n})=0.5$$的数据点颜色为青色。

如图9-5(c)所示。

将9-5(c)与9-5(a)进行对比，可以看出哪些数据点由其被生成的真实分量进行标记。

就该图而论，每个数据点的颜色中的红蓝绿三个颜色分量表示了各个分量的responsibility。

# 三、最大似然

假设我们有一个观测的数据集$$\{\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_N\}$$($$\mathbf{x_j}$$为D维向量),我们希望使用混合高斯模型来对数据进行建模。我们可以将这个数据集表示为一个N × D的矩阵$$\mathbf{X}$$,其中第n行为$$\mathbf{x}_n^{T}$$ 。类似地,对应的隐含变量会被表示为一个N × K的矩阵$$\mathbf{Z}$$,它的行为$$\mathbf{z}_n^{T}$$。

如果我们假定数据点独立地从概率分布中抽取，那么

$$p(\mathbf{X})=p(\mathbf{x}_1)*p(\mathbf{x}_2)....p(\mathbf{x}_N)\\
=\prod_{n=1}^{N}\left( \sum_{k=1}^{K}\pi_k N(\mathbf{x} \vert \mu_k ,\Sigma_k) \right)\\
\Rightarrow\\
lnp (\mathbf{X} \vert \pi ,\mu ,\Sigma )=\sum_{n=1}^{N}ln \left(  \sum_{k=1}^{K} \pi_k N(x_n \vert \mu_k , \Sigma_k) \right)
$$

在我们讨论如何最大化这个函数之前,有必要强调一下由于奇异性的存在,造成的应用于高斯混合模型的最大似然框架中的一个大问题。为了简化起见,我们考虑一个高斯混合模型,它的分量的协方差矩阵为$$\Sigma_k=\sigma_k^2 \mathbf{I}$$,虽然其中I是一个单位矩阵,但是结论对于一般的协方差矩阵仍然成立。

假设混合模型的第j个分量的均值μj与某个数据点完全相同,即对于某 个n值,$$\mu_j = x_n$$。这样,这个数据点会为似然函数贡献一项,形式为

$$N(\mathbf{x_n} \vert \mathbf{x_n} , \sigma_j^2 \mathbf{I})=\frac{1}{(2\pi)^{\frac{1}{2}}} \frac{1}{\sigma_j^D}$$



## 1.最大似然的困难点

如果我们考虑极限 $$\sigma_j \to 0$$,那么我们看到这一项趋于无穷大,因此对数似然函数也会趋于无穷 大。因此,对数似然函数的最大化不是一个具有良好定义的问题,因为这种奇异性总会出现, 会发生在任何一个“退化”到一个具体的数据点上的高斯分量上。

由于奇异点的存在导致的诸多问题请参考PRML9.2.1

# 四、高斯混合的参数求值

一种优雅的并且强大的寻找带有*`隐含变量`*的模型的最大似然解的方法被称为期望最大化算法(expectation-maximization algorithm),或者EM算法。

我们要强调的时,EM算法具有广泛的适用性,实际上在本书中讨论的许多不 同模型中都会遇到它。


## 1.引申——有关高斯混合的各种偏导的计算

高斯混合的表达式，注意此处的符号记法与上述略有不同，主要是混合系数用$$\rho_k$$而非$$\pi_k$$

<img src="/images/PRML/EM/image3.png"  >

高斯混合由三个参数决定，即$$\mathbf{\pi},\mathbf{\mu},\mathbf{\Sigma}$$。其对于这三个参数的偏导如下：

<img src="/images/PRML/EM/image4.png"  >

首先,让我们写下似然函数的最大值必须满足的条件。

## 2.高斯分量的均值
令公式(9.14)中$$lnp (\mathbf{X} \vert \pi ,\mu ,\Sigma )$$关于高斯分量的均值μk 的导数等于零,我们有

<img src="/images/PRML/EM/image2.png"  >

将上式左右乘以$$\Sigma_k$$（假设矩阵是非奇异的），整理可得：

$$
\sum_{n=1}^{N}\gamma(z_{nk})\mathbf{x}_n=\sum_{n=1}^{N}\gamma(z_{nk}) \mathbf{\mu}_n\\
\Rightarrow\\
\mu_k=\frac{1}{\sum_{n=1}^{N}\gamma(z_{nk})}\sum_{n=1}^{N}\gamma(z_{nk})\mathbf{x}_n\\
=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\mathbf{x}_n
$$

其中我们定义:

$$N_k=\sum_{n=1}^{N}\gamma(z_{nk})$$

> 我们可以将第k个高斯分量的均值$$\mu_k$$视为数据集中所有数据点的加权平均，每个数据点的权重由其后验概率(responsibility)——$$\gamma(z_{nk})$$表示，而$$N_k$$则可以视为这些权重之和。

进一步地：

> 我们可以将$$N_k$$视为被分配到簇k的数据点的有效数量。

## 3.高斯分量的协方差

我们将$$lnp(\mathbf{X} \vert \mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma})$$关于$$\Sigma_k$$求偏导并令其等于0：

$$\Sigma_k=\frac{1}{N_k}\sum_{n=1}^{N} \gamma(z_{nk})(\mathbf{x}_n- \mathbf{u}_k)(\mathbf{x}_n- \mathbf{u}_k)^T$$

这与一元高斯分布的对应的结果具有相同的函数形式(见2.3.4),但是与之前一样,每个数据点都有一个权值,权值等于对应的后验概率(responsibility)——$$\gamma(z_nk)$$

## 4.高斯分量的混合系数

最后，我们将$$lnp(\mathbf{X} \vert \mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma})$$关于混合系数$$\pi_k$$求偏导并令其等于0。

这里我们必须考虑限制条件(9.9), 它要求混合系数的加和等于1，即$$\sum_{k=1}^{K}\pi_k=1$$。使用拉格朗日乘数法,最大化下面的量：

$$lnp(\mathbf{X} \vert \mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}) + \lambda \left( \sum_{k=1}^{K} \pi_k -1 \right)\\
\Rightarrow\\
0=\sum_{n=1}^{N} \frac{N(x_n \vert \mu_k ,\Sigma_k)}{\sum_j \pi_j N(x_n \vert \mu_j ,\Sigma_j)} +\lambda
$$

将上式左右两边乘以$$\pi_k$$，凑出$$\gamma(z_{nk})$$:

$$0=\sum_{n=1}^{N} \gamma(z_{nk}) + \lambda \pi_k \\
\Rightarrow \\
-\lambda \pi_k=\sum_{n=1}^{N} \gamma(z_{nk}) \\
\Rightarrow \\
-\lambda \sum_{k=1}^{K}\pi_k= \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{nk})\\
\Rightarrow \\
-\lambda=N\\
\Rightarrow \\
\lambda=-N
$$

将$$\lambda=-N$$进行回代，

$$0=\sum_{n=1}^{N} \gamma(z_{nk}) + \lambda \pi_k \\
\Rightarrow \\
0=N_k-N\pi_k \\
\Rightarrow \\
\pi_k=\frac{N_k}{N}
$$

> 因为
>
$$\pi_k=\frac{\sum_{n=1}^{N}  \gamma(z_{nk})}{N}$$
>所以第k个分量的混合系数可以认为是该分量对所有数据点的responsibility的平均值。

## 5.高斯混合与responsibility的关系

从上面的推导可以看出，高斯混合与单个高斯分布的参数形式(均值与协方差)极其相似，只是高斯混合的参数是以$$\gamma(z_{nk})$$为权重的加权。

值得强调的是，高斯混合的混合系数、均值，协方差矩阵等虽然可以由上述公式所述那样由$$\gamma(z_{nk})$$得到，但并不能为这些参数得到一个解析解。因为$$\gamma(z_{nk})$$本身也以一种复杂的方式依赖于这些参数：


$$\gamma(z_k)=p(z_k=1 \vert x)=\frac{p(z_k)=1p(x \vert z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(x \vert z_j =1)}\\
=\frac{\pi_k N(x \vert \mu_k , \Sigma_k)}{\sum_{j=1}^{K} \pi_j N(x \vert \mu_j , \Sigma_j)}
$$




# 五.EM算法

从上述$$\gamma(z_{nk})$$与混合系数、均值、协方差矩阵的相互关系，我们可以用一种两步迭代方法来找到最大似然解。这里我们要用EM算法。

EM算法分为E步骤和M步骤：

1. 在E步骤（expectation step，期望步骤）中，根据均值、协方差、混合系数来计算后验概率(responsibility——$$\gamma(z_{nk})$$)

2. 在M步骤(maximization step，最大化步骤)中，根据responsibility来计算均值、协方差、混合系数。注意在这一步中，我们先计算出均值，然后根据均值来计算协方差。

>我们稍后会证明,每次通过E步 骤和接下来的M步骤对参数的更新确保了对数似然函数的增大。在实际应用中,当对数似然函 数的变化量或者参数的变化量低于某个阈值时,我们就认为算法收敛。

## 1.根据K-Means算法来为EM算法初始化

在运行EM算法之前，首先要为均值、协方差、混合系数设置一个初始值。

注意,与K均值算法相比,EM算法在达到(近似)收敛之前,经历了更多次的迭代,每次 迭代需要更多的计算量。因此,通常运行K均值算法找到高斯混合模型的一个合适的初始化 值,接下来使用EM算法进行调节。

> 1.协方差矩阵可以很方便地初始化为通过K 均值算法找到的聚 类的样本协方差

> 2.混合系数可以被设置为分配到对应类别中的数据点所占的比例。

## 2.EM算法的收敛性

该强调的是,通常对数似然函数会有多个局部极大值,EM不保证找到这些极大值中最大的一个

> EM算法只能找到局部最优解，而不能找到全局最优解

## 3.EM算法的总结

### 1.初始化均值$$\mu_k$$、协方差$$\Sigma_k$$和混合系数$$\pi_k$$,计算对数似然函数的初始值。

### 2.E步骤。使用当前参数值计算“responsibility”:
 
$$\gamma(z_{nk})=\frac{\pi_k N(x_n \vert \mu_k , \Sigma_k)}{\sum_{j=1}^{K} \pi_j N(x_n \vert \mu_j , \Sigma_j)}
$$

### 3.M步骤。使用当前的“responsibility”重新估计参数。

$$\mu_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\mathbf{x}_n$$

$$\Sigma_k^{new}=\frac{1}{N_k}\sum_{n=1}^{N} \gamma(z_{nk})(\mathbf{x}_n- \mathbf{u}_k^{new})(\mathbf{x}_n- \mathbf{u}_k^{new})^T$$

$$\pi_k^{new}=\frac{N_k}{N}$$

其中：

$$N_k=\sum_{n=1}^{N}\gamma(z_{nk})$$

### 4.计算对数似然函数:

$$lnp (\mathbf{X} \vert \pi ,\mu ,\Sigma )=\sum_{n=1}^{N}ln \left(  \sum_{k=1}^{K} \pi_k N(x_n \vert \mu_k , \Sigma_k) \right)$$

##### 检查参数或者对数似然函数的收敛性。如果没有满足收敛的准则,则返回第2步。


```python

```
